{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f44ee7-ead0-4bc6-8588-396d83b10fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers==4.39.0\n",
    "%pip install -q peft==0.12.0\n",
    "%pip install -q datasets==2.20.0\n",
    "%pip install -q evaluate==0.4.2\n",
    "%pip install -q accelerate==0.34.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1178895-08b9-4ef3-956c-68f657683526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef48aec2-4402-4954-b519-2581e4b63fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset labels: {0, 1, 2, 3, 4}\n",
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "print(f\"Dataset labels: {set(dataset['train']['label'])}\")\n",
    "print(f\"Number of classes: {len(set(dataset['train']['label']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9ba181-0a0d-4d57-89d2-d6a655f302be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select smaller subsets for training and evaluation\n",
    "training_dataset = dataset[\"train\"].shuffle(seed=42).select(range(500))\n",
    "evaluation_dataset = dataset[\"test\"].shuffle(seed=42).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "727615be-4886-49c9-8d13-6b21877b82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "num_labels = 5  # Yelp has 5 classes (0-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610bb596-9530-49b7-9020-e348b184e2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\abc\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with correct number of labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels,\n",
    "    id2label={0: \"1 star\", 1: \"2 stars\", 2: \"3 stars\", 3: \"4 stars\", 4: \"5 stars\"},\n",
    "    label2id={\"1 star\": 0, \"2 stars\": 1, \"3 stars\": 2, \"4 stars\": 3, \"5 stars\": 4}\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90bcf5da-0684-46f9-b741-33c0c4ec18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_toke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe3d7c9-e298-41c7-897d-4db2090400ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=True,  # We'll use data collator for dynamic padding\n",
    "        truncation=True, \n",
    "        max_length=512,\n",
    "        return_tensors=None  # Return as lists, not tensors\n",
    "    )\n",
    "    \n",
    "    # Add labels to the output\n",
    "    tokenized[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa5f465-cffe-45c4-a33a-67dba7b5c733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e6b94d0c1447849d2d1f2f0cc9b7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbfdbabc961498ea42aacd0d64e8fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize datasets\n",
    "tokenized_train = training_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = evaluation_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05879870-4fc2-49c9-888a-293423cf28d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the original text column to save memory\n",
    "tokenized_train = tokenized_train.remove_columns([\"text\"])\n",
    "tokenized_eval = tokenized_eval.remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e159819-666c-4292-9b80-fa313e2cefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set format for PyTorch\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_eval.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbc5dae0-0fc1-466f-b5dc-769c87bab2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\", \"key\", \"dense\"],  # Added more target modules\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5d35171-8fb4-44d6-875e-95ab860551ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,273,221 || all params: 127,947,274 || trainable%: 2.5583\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d041434c-ee90-4dc6-9476-c803f99739a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52528055-c99f-4df7-a0a1-8dc1778a8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"roberta_peft_yelp\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,  # Slightly higher learning rate for LoRA\n",
    "    per_device_train_batch_size=4,  # Reduced batch size\n",
    "    per_device_eval_batch_size=4,\n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_accumulation_steps=2,  # Effective batch size of 8\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=1.0,  # More conservative gradient clipping\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,  # Add warmup\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    dataloader_pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c97c13b0-7ea1-448c-9a18-9a83e8df5dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\abc\\lib\\site-packages\\accelerate\\accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    "    # Removed compute_metrics parameter\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1be3ec3d-a9bd-4209-94d6-1773e0c92358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\abc\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\User\\miniconda3\\envs\\abc\\lib\\site-packages\\torch\\utils\\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='186' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [186/186 43:57, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.635800</td>\n",
       "      <td>1.599941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.562400</td>\n",
       "      <td>1.586587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\abc\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\User\\miniconda3\\envs\\abc\\lib\\site-packages\\torch\\utils\\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\miniconda3\\envs\\abc\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\miniconda3\\envs\\abc\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\User\\miniconda3\\envs\\abc\\lib\\site-packages\\torch\\utils\\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\miniconda3\\envs\\abc\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=186, training_loss=1.6049273296069073, metrics={'train_runtime': 2645.4532, 'train_samples_per_second': 0.567, 'train_steps_per_second': 0.07, 'total_flos': 406594474573824.0, 'train_loss': 1.6049273296069073, 'epoch': 2.98})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a72b6132-c5f3-459b-9f1e-a21adcc61d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 06:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.5865873098373413, 'eval_runtime': 406.7429, 'eval_samples_per_second': 1.229, 'eval_steps_per_second': 0.307, 'epoch': 2.98}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model (will only show loss without accuracy)\n",
    "print(\"Evaluating model...\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e05b922-6616-42c2-9373-ba97432fd10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\miniconda3\\envs\\abc\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "trainer.save_model(\"roberta_peft_yelp_final\")\n",
    "tokenizer.save_pretrained(\"roberta_peft_yelp_final\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14ea60dc-e29a-4a29-bf9c-3556b208c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "778aba58-87b5-4dbd-a8de-e47ba94a6cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PEFT model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Path to your fine-tuned PEFT model\n",
    "model_path = r\"C:\\Users\\User\\Desktop\\ABC\\roberta_peft_yelp_final\"\n",
    "\n",
    "# Device (CPU)\n",
    "device = \"cpu\"\n",
    "\n",
    "try:\n",
    "    # Load base Roberta model\n",
    "    base_model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "    \n",
    "    # Load PEFT adapters on top\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"✅ PEFT model loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Loading failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6bc20d-00f1-4c5e-88f4-cd2d3581ab99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Yelp Review Sentiment Analyzer (CPU mode)\n",
      "Type 'quit' to exit\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Enter your review:  This product is so good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Sentiment: Neutral 😐\n",
      "⭐ Rating: ⭐⭐⭐\n",
      "📊 Confidence: 26.6%\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Enter your review:  Eww i don't like this product at all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Sentiment: Neutral 😐\n",
      "⭐ Rating: ⭐⭐⭐\n",
      "📊 Confidence: 26.2%\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Enter your review:  bad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Sentiment: Neutral 😐\n",
      "⭐ Rating: ⭐⭐⭐\n",
      "📊 Confidence: 26.3%\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Enter your review:  ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Sentiment: Neutral 😐\n",
      "⭐ Rating: ⭐⭐⭐\n",
      "📊 Confidence: 26.2%\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# CPU device\n",
    "device = \"cpu\"\n",
    "\n",
    "# Simple linear classifier on top of CLS embedding\n",
    "classifier = nn.Linear(model.base_model.config.hidden_size, 5).to(device)\n",
    "classifier.eval()  # eval mode\n",
    "\n",
    "rating_map = {\n",
    "    0: (\"Very Negative 😠\", \"⭐\"),\n",
    "    1: (\"Negative 😞\", \"⭐⭐\"),\n",
    "    2: (\"Neutral 😐\", \"⭐⭐⭐\"),\n",
    "    3: (\"Positive 🙂\", \"⭐⭐⭐⭐\"),\n",
    "    4: (\"Very Positive 😄\", \"⭐⭐⭐⭐⭐\")\n",
    "}\n",
    "\n",
    "def analyze_sentiment():\n",
    "    print(\"🤖 Yelp Review Sentiment Analyzer (CPU mode)\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\n📝 Enter your review: \").strip()\n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye! 👋\")\n",
    "            break\n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                user_input,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Forward pass through the PEFT model but only get hidden states\n",
    "                outputs = model.base_model(**inputs)  # NOTE: use base_model\n",
    "                cls_emb = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "                \n",
    "                # Pass through linear classifier\n",
    "                logits = classifier(cls_emb)\n",
    "                predictions = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                \n",
    "                predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "                confidence = predictions[0][predicted_class].item()\n",
    "            \n",
    "            sentiment_text, stars = rating_map[predicted_class]\n",
    "            print(f\"\\n🎯 Sentiment: {sentiment_text}\")\n",
    "            print(f\"⭐ Rating: {stars}\")\n",
    "            print(f\"📊 Confidence: {confidence:.1%}\")\n",
    "            print(\"=\" * 40)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "\n",
    "# Run analyzer\n",
    "analyze_sentiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33bfd0-2248-473d-ae41-f7c48f2c6cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
